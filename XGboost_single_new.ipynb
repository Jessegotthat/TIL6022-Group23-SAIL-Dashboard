{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "import warnings, sys, os, random\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e365684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"XGBoost:\", xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7707865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your SAIL dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (use your actual file path if different)\n",
    "df = pd.read_csv(\"SAIL2025_LVMA_data_3min_20August-25August2025_flow.csv\")\n",
    "\n",
    "# Step 2: Define features (X) and a single target (y)\n",
    "# Features = time-related columns\n",
    "X = df[[\"hour\", \"minute\", \"day\", \"weekday\", \"is_weekend\"]]\n",
    "\n",
    "# --- NEW PART ---\n",
    "# Select one specific sensor column as the prediction target\n",
    "# You can manually change the name below to any column you want to test\n",
    "TARGET_SENSOR = \"CMSA-GAKH-01_0\"   # <- replace with your actual sensor column name\n",
    "y = df[TARGET_SENSOR]\n",
    "# --- END ---\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "print(\"Selected target sensor:\", TARGET_SENSOR)\n",
    "\n",
    "# Optional: preview\n",
    "display(df[[TARGET_SENSOR]].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3) Add lag features for the selected sensor =====\n",
    "import pandas as pd\n",
    "\n",
    "def create_lag_features(series: pd.Series, lags) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build lag features from a target series.\n",
    "    Each lag k creates a column: <series.name>_lag{k} = series.shift(k)\n",
    "    \"\"\"\n",
    "    lag_df = pd.concat({f\"{series.name}_lag{k}\": series.shift(k) for k in lags}, axis=1)\n",
    "    return lag_df\n",
    "\n",
    "# Choose how many past steps to include as predictors\n",
    "# Each \"step\" corresponds to one row in your dataset (e.g., 3 minutes in your file)\n",
    "LAG_STEPS = [1, 2, 3, 4, 5, 6]  # start simple; you can test [1,2,3] first if you like\n",
    "\n",
    "# 1) Create lag features for the selected target sensor\n",
    "lag_features = create_lag_features(df[TARGET_SENSOR], LAG_STEPS)\n",
    "\n",
    "# 2) Concatenate time features (X) with lag features\n",
    "X_lagged = pd.concat([X, lag_features], axis=1)\n",
    "\n",
    "# 3) Align X and y by dropping rows with NaNs introduced by shifting\n",
    "aligned = pd.concat([X_lagged, y], axis=1).dropna()\n",
    "\n",
    "# 4) Final matrices for modeling\n",
    "X_final = aligned[X_lagged.columns]\n",
    "y_final = aligned[TARGET_SENSOR]\n",
    "\n",
    "print(\"X_final shape:\", X_final.shape)\n",
    "print(\"y_final shape:\", y_final.shape)\n",
    "display(X_final.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d954e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4) Time-based split =====\n",
    "# Keep chronological order; do NOT shuffle for time series\n",
    "test_ratio = 0.2\n",
    "test_size = int(len(X_final) * test_ratio)\n",
    "\n",
    "X_train, X_test = X_final.iloc[:-test_size], X_final.iloc[-test_size:]\n",
    "y_train, y_test = y_final.iloc[:-test_size], y_final.iloc[-test_size:]\n",
    "\n",
    "print(\"Train size:\", X_train.shape, y_train.shape)\n",
    "print(\"Test size :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eaa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5) Train XGBoost regressor =====\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"   # good default on CPU\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6) Predict and evaluate =====\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "# Compute RMSE in a version-agnostic way\n",
    "mse  = mean_squared_error(y_test, y_pred)      # squared=True by default\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7) Naive baseline (persistence) for reference =====\n",
    "# Predict the next value equals the last observed value (lag-1)\n",
    "naive_pred = X_test[f\"{TARGET_SENSOR}_lag1\"].values\n",
    "\n",
    "r2_naive  = r2_score(y_test, naive_pred)\n",
    "mae_naive = mean_absolute_error(y_test, naive_pred)\n",
    "mse_naive  = mean_squared_error(y_test, naive_pred)\n",
    "rmse_naive = np.sqrt(mse_naive)\n",
    "\n",
    "print(\"[Naive baseline]\")\n",
    "print(f\"R2:   {r2_naive:.4f}\")\n",
    "print(f\"MAE:  {mae_naive:.4f}\")\n",
    "print(f\"RMSE: {rmse_naive:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8) Quick plot (actual vs prediction on test set) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_test.index, y_test.values, label=\"Actual\")\n",
    "plt.plot(y_test.index, y_pred, label=\"XGB\")\n",
    "plt.legend()\n",
    "plt.title(f\"{TARGET_SENSOR} - Test set (Actual vs XGB)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Flow\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0214720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Utilities for time series CV with a gap and for metrics ====\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def time_split_with_gap(X: pd.DataFrame,\n",
    "                        y: pd.Series,\n",
    "                        test_ratio: float = 0.2,\n",
    "                        gap: int = 0):\n",
    "    \"\"\"\n",
    "    Split a time-series dataset into training and validation sets,\n",
    "    leaving a gap period between them to prevent data leakage from lag features.\n",
    "\n",
    "    Parameters:\n",
    "        X : pd.DataFrame\n",
    "            Feature matrix sorted in chronological order.\n",
    "        y : pd.Series\n",
    "            Target series aligned with X.\n",
    "        test_ratio : float, optional (default=0.2)\n",
    "            Proportion of samples used for the validation set.\n",
    "        gap : int, optional (default=0)\n",
    "            Number of observations to skip between train and validation sets.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_valid, y_train, y_valid\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    test_size = int(n * test_ratio)\n",
    "    # The split point is set before the validation set, leaving the gap\n",
    "    split_point = n - test_size - gap\n",
    "    if split_point <= 0:\n",
    "        raise ValueError(f\"split_point <= 0, reduce test_ratio or gap. n={n}, test_size={test_size}, gap={gap}\")\n",
    "\n",
    "    X_train = X.iloc[:split_point]\n",
    "    y_train = y.iloc[:split_point]\n",
    "    # Validation set starts after the gap until the end\n",
    "    X_valid = X.iloc[split_point + gap:]\n",
    "    y_valid = y.iloc[split_point + gap:]\n",
    "    return X_train, X_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions using common regression metrics.\n",
    "\n",
    "    Returns:\n",
    "        r2   : RÂ² score\n",
    "        mae  : Mean Absolute Error\n",
    "        rmse : Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return r2, mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed524826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Optuna: also optimize the number of lags (max_lag) =====\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# cache to avoid rebuilding features for the same max_lag repeatedly\n",
    "_LAG_CACHE = {}  # key: max_lag -> (X_final, y_final)\n",
    "\n",
    "def build_dataset_with_lags(max_lag: int) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Build X_final, y_final for a given max_lag and cache the result.\n",
    "    Lags used are contiguous: 1..max_lag.\n",
    "    \"\"\"\n",
    "    if max_lag in _LAG_CACHE:\n",
    "        return _LAG_CACHE[max_lag]\n",
    "\n",
    "    # 1) lag block\n",
    "    lag_features = create_lag_features(df[TARGET_SENSOR], range(1, max_lag + 1))\n",
    "    # 2) concat time features + lags\n",
    "    X_lagged = pd.concat([X, lag_features], axis=1)\n",
    "    # 3) align and drop NaNs due to shift\n",
    "    aligned = pd.concat([X_lagged, y], axis=1).dropna()\n",
    "    X_final = aligned[X_lagged.columns]\n",
    "    y_final = aligned[TARGET_SENSOR]\n",
    "\n",
    "    _LAG_CACHE[max_lag] = (X_final, y_final)\n",
    "    return X_final, y_final\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Jointly optimize:\n",
    "      - max_lag (how many past steps to include)\n",
    "      - key XGBoost hyperparameters\n",
    "    Target: maximize R2 on a chronological validation split with a leakage-safe gap.\n",
    "    \"\"\"\n",
    "    # --- search space ---\n",
    "    max_lag = trial.suggest_int(\"max_lag\", 3, 12)  # adjust bounds to your cadence/horizon\n",
    "\n",
    "    params = dict(\n",
    "        n_estimators     = trial.suggest_int(\"n_estimators\", 300, 1000),\n",
    "        learning_rate    = trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "        max_depth        = trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 12),\n",
    "        subsample        = trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "        gamma            = trial.suggest_float(\"gamma\", 0.0, 3.0),\n",
    "        reg_lambda       = trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        reg_alpha        = trial.suggest_float(\"reg_alpha\", 0.0, 2.0),\n",
    "        random_state     = 42,\n",
    "        tree_method      = \"hist\",\n",
    "    )\n",
    "\n",
    "    # --- dataset for this lag setting (cached) ---\n",
    "    Xf, yf = build_dataset_with_lags(max_lag)\n",
    "\n",
    "    # chronological split; gap = max_lag to be safe\n",
    "    X_train, X_valid, y_train, y_valid = time_split_with_gap(Xf, yf, test_ratio=0.2, gap=max_lag)\n",
    "\n",
    "    # --- fit & eval ---\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    r2, mae, rmse = evaluate_preds(y_valid, y_pred)\n",
    "\n",
    "    # store aux metrics to the trial\n",
    "    trial.set_user_attr(\"mae\", mae)\n",
    "    trial.set_user_attr(\"rmse\", rmse)\n",
    "    trial.set_user_attr(\"n_valid\", len(y_valid))\n",
    "\n",
    "    return r2  # maximize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b45e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Run study =====\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_sail_joint_lag\")\n",
    "study.optimize(objective, n_trials=40, show_progress_bar=True)  # start with 30-40; scale up if needed\n",
    "\n",
    "print(\"Best R2:\", f\"{study.best_value:.4f}\")\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nAux metrics (best trial):\")\n",
    "print(\"  MAE :\", f\"{study.best_trial.user_attrs['mae']:.4f}\")\n",
    "print(\"  RMSE:\", f\"{study.best_trial.user_attrs['rmse']:.4f}\")\n",
    "print(\"  n_valid:\", study.best_trial.user_attrs['n_valid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41496d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Train final model with the best max_lag and best hyperparams =====\n",
    "best = study.best_trial.params\n",
    "best_max_lag = best.pop(\"max_lag\")  # separate lag count from model params\n",
    "\n",
    "Xf_best, yf_best = build_dataset_with_lags(best_max_lag)\n",
    "X_train, X_valid, y_train, y_valid = time_split_with_gap(Xf_best, yf_best, test_ratio=0.2, gap=best_max_lag)\n",
    "\n",
    "best_model = XGBRegressor(**best, random_state=42, tree_method=\"hist\")\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_best = best_model.predict(X_valid)\n",
    "\n",
    "r2, mae, rmse = evaluate_preds(y_valid, y_pred_best)\n",
    "print(\"\\n[Final evaluation with best max_lag]\")\n",
    "print(\"max_lag:\", best_max_lag)\n",
    "print(\"R2  :\", f\"{r2:.4f}\")\n",
    "print(\"MAE :\", f\"{mae:.4f}\")\n",
    "print(\"RMSE:\", f\"{rmse:.4f}\")\n",
    "\n",
    "# naive baseline on the same validation set (persistence)\n",
    "naive_pred = X_valid[f\"{TARGET_SENSOR}_lag1\"].values\n",
    "r2_b, mae_b, rmse_b = evaluate_preds(y_valid, naive_pred)\n",
    "print(\"\\n[Naive baseline]\")\n",
    "print(\"R2  :\", f\"{r2_b:.4f}\")\n",
    "print(\"MAE :\", f\"{mae_b:.4f}\")\n",
    "print(\"RMSE:\", f\"{rmse_b:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88263a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Viz 1: Optuna optimization history (trial vs R2) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# collect R2 per trial\n",
    "trial_nums = [t.number for t in study.trials if t.value is not None]\n",
    "trial_values = [t.value for t in study.trials if t.value is not None]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(trial_nums, trial_values, marker=\"o\")\n",
    "plt.title(\"Optuna: Validation R^2 over trials\")\n",
    "plt.xlabel(\"Trial\")\n",
    "plt.ylabel(\"Validation R^2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bdafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Viz 2: Best model - Actual vs Predicted on validation set =====\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_valid.index, y_valid.values, label=\"Actual\")\n",
    "plt.plot(y_valid.index, y_pred_best, label=\"XGB (best)\")\n",
    "plt.title(f\"{TARGET_SENSOR} - Validation set (Actual vs Prediction)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Flow\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residuals (y - y_hat)\n",
    "resid = y_valid.values - y_pred_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f79b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Viz 3: Residual diagnostics =====\n",
    "# 3a) Residuals over time\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(y_valid.index, resid)\n",
    "plt.title(\"Residuals over time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3b) Residuals histogram\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(resid, bins=30)\n",
    "plt.title(\"Residuals distribution\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b46258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Viz 4: Feature importance (XGBoost) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get feature importances from the trained best model\n",
    "fi = best_model.feature_importances_\n",
    "feat_names = list(X_train.columns)\n",
    "\n",
    "fi_df = pd.DataFrame({\"feature\": feat_names, \"importance\": fi})\n",
    "fi_df = fi_df.sort_values(\"importance\", ascending=True).tail(20)  # top 20\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(fi_df[\"feature\"], fi_df[\"importance\"])\n",
    "plt.title(\"Feature importance (top 20)\")\n",
    "plt.xlabel(\"Gain-based importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
