{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d8e61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core scientific stack ===\n",
    "import sys, os, re, json, time, random, warnings, logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "\n",
    "# === Scikit-learn (keep only whatâ€™s needed for time-series and metrics) ===\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "# === Optuna for hyperparameter optimization ===\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# === Persistence & utilities ===\n",
    "import joblib\n",
    "from tqdm import tqdm  # progress bar for multi-sensor training\n",
    "\n",
    "# Global configuration\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Use half of available CPU cores for XGBoost\n",
    "N_JOBS = os.cpu_count()\n",
    "\n",
    "# Output directories for saving models, figures, and metrics\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "FIGS_DIR = Path(\"figures\")\n",
    "for d in [ARTIFACTS_DIR, MODELS_DIR, FIGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Basic logging setup â€” helpful when training multiple sensors\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)   # åªä¿ç•™ warning ä»¥ä¸Š\n",
    "optuna.logging.disable_default_handler()               # å…³æ‰ Optuna è‡ªå·±çš„ stdout handler\n",
    "xgb.set_config(verbosity=0)  \n",
    "\n",
    "# Suppress non-critical warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0783c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.9\n",
      "NumPy: 2.3.3\n",
      "Pandas: 2.3.2\n",
      "scikit-learn: 1.7.2\n",
      "XGBoost: 3.0.1\n",
      "Optuna: 4.5.0\n",
      "CPU cores: 22 | N_JOBS: 22\n"
     ]
    }
   ],
   "source": [
    "# Version check\n",
    "import sklearn\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"XGBoost:\", xgb.__version__)\n",
    "print(\"Optuna:\", optuna.__version__)\n",
    "print(\"CPU cores:\", os.cpu_count(), \"| N_JOBS:\", N_JOBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb23c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:41:27,639 | INFO | Rows: 2400 | Sensors: 37 | Time features: 6 | Exogenous: 6\n",
      "2025-11-05 16:41:27,639 | INFO | Sample sensors: ['CMSA-GAKH-01__sensor', 'CMSA-GAWW-11__sensor', 'CMSA-GAWW-12__sensor', 'CMSA-GAWW-13__sensor', 'CMSA-GAWW-14__sensor']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# å…¨å±€è®¾ç½®ï¼ˆåˆå¹¶ç‰ˆï¼‰ï¼šè·¯å¾„ / é…ç½® / æ•°æ®è¯»å– / åˆ—è‡ªåŠ¨è¯†åˆ«\n",
    "# è¯´æ˜ï¼š\n",
    "# - ä½¿ç”¨ä¸€ä¸ª CFG å­—å…¸é›†ä¸­ç®¡ç†è°ƒå‚ä¸åˆ‡åˆ†å¼€å…³\n",
    "# - ç»Ÿä¸€åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "# - è¯»å–CSVå¹¶è®¾ç½®æ—¶é—´ç´¢å¼•\n",
    "# - è‡ªåŠ¨è¯†åˆ«ï¼šä¼ æ„Ÿå™¨åˆ— / æ—¶é—´æ´¾ç”Ÿç‰¹å¾ / å¤–ç”Ÿå˜é‡åŠå…¶ isna æŒ‡ç¤º\n",
    "# - å»æ‰é‡å¤å˜é‡å‘½åï¼ˆä»…ä¿ç•™ TIME_COLS / EXOG_ALL / SENSOR_COLSï¼‰\n",
    "# ============================================================\n",
    "\n",
    "# ---------- 1) è¾“å…¥æ–‡ä»¶ ----------\n",
    "# æ³¨ï¼šå¦‚éœ€ç›¸å¯¹è·¯å¾„å¯æ”¹ä¸º Path(\"training_dataset_tomtom_vessel_ready.csv\")\n",
    "DATA_PATH = Path(\"C:/Users/elvinli/OneDrive/CodeProjects/TIL6022-Group23-SAIL-Dashboard/outputs_fast/training_dataset_tomtom_vessel_ready.csv\")\n",
    "\n",
    "# ---------- 2) è¾“å‡ºç›®å½• ----------\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "FIGS_DIR = Path(\"figures\")\n",
    "for d in [ARTIFACTS_DIR, MODELS_DIR, FIGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- 3) ç»Ÿä¸€é…ç½® ----------\n",
    "# ä¾èµ–ï¼šSEED / N_JOBS å·²åœ¨å¯¼å…¥å•å…ƒä¸­å®šä¹‰\n",
    "CFG = {\n",
    "    # éšæœºæ€§ä¸å¹¶è¡Œ\n",
    "    \"seed\": SEED,\n",
    "    \"n_jobs\": N_JOBS,\n",
    "\n",
    "    # æ•°æ®åˆ‡åˆ†\n",
    "    \"test_ratio\": 0.20,          # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆæŒ‰æ—¶é—´æœ«å°¾åˆ‡åˆ†ï¼‰\n",
    "    \"target_shift\": 0,           # è‹¥è¦é¢„æµ‹ t+1ï¼Œå¯è®¾ä¸º 1ï¼›å½“å‰ä¿æŒ 0 é¢„æµ‹å½“æœŸ\n",
    "\n",
    "    # Optuna è°ƒå‚\n",
    "    \"use_optuna\": True,          # æ˜¯å¦ä¸ºæ¯ä¸ªä¼ æ„Ÿå™¨å¼€å¯è°ƒå‚\n",
    "    \"optuna_n_trials\": 50,       # æ¯ä¸ªä¼ æ„Ÿå™¨çš„ trial æ•°ï¼ˆå…ˆè½»é‡ï¼Œåç»­å¯åŠ å¤§ï¼‰\n",
    "    \"trial_timeout_sec\": 120,    # æ¯ä¸ª trial çš„æœ€é•¿æ—¶é—´ï¼Œé¿å…å¡æ­»\n",
    "    \"early_stopping_rounds\": 100,# XGB çš„ early stopping\n",
    "\n",
    "    # æ»åä¸é˜²æ³„æ¼\n",
    "    \"default_max_lag\": 6,        # è‹¥ä¸æœç´¢ max_lagï¼Œç”¨è¯¥é»˜è®¤å€¼\n",
    "    \"gap_equals_max_lag\": True,  # éªŒè¯å‰çš„ gap = max_lagï¼Œä¸¥æ ¼é˜²æ³„æ¼\n",
    "}\n",
    "\n",
    "# ---------- 4) XGBoost é»˜è®¤å‚æ•°ï¼ˆOptunaä¼šåœ¨æ­¤åŸºç¡€ä¸Šæœç´¢/è¦†ç›–ï¼‰ ----------\n",
    "XGB_BASE_PARAMS = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_estimators\": 500,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "    \"random_state\": CFG[\"seed\"],\n",
    "    \"n_jobs\": CFG[\"n_jobs\"],\n",
    "}\n",
    "\n",
    "# ---------- 5) è¯»å–æ•°æ®å¹¶è®¾ç½®æ—¶é—´ç´¢å¼• ----------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
    "\n",
    "# ---------- 6) è‡ªåŠ¨è¯†åˆ«åˆ—ï¼ˆä»…é€‰æ‹©å­˜åœ¨çš„åˆ—ï¼Œé¿å… KeyErrorï¼‰ ----------\n",
    "all_cols = df.columns.tolist()\n",
    "\n",
    "# 6.1 ä¼ æ„Ÿå™¨åˆ—ï¼ˆæŒ‰ä½ çš„å‘½åçº¦å®šï¼šä»¥ \"__sensor\" ç»“å°¾ï¼‰\n",
    "SENSOR_COLS = [c for c in all_cols if c.endswith(\"__sensor\")]\n",
    "\n",
    "# 6.2 æ—¶é—´æ´¾ç”Ÿç‰¹å¾ï¼ˆåªä¿ç•™åœ¨æ•°æ®è¡¨ä¸­ç¡®å®å­˜åœ¨çš„ï¼‰\n",
    "_CANDIDATE_TIME = [\"hour\", \"minute\", \"weekday\", \"is_weekend\", \"week\", \"minute_of_day\"]\n",
    "TIME_COLS = [c for c in _CANDIDATE_TIME if c in df.columns]\n",
    "\n",
    "# 6.3 å¤–ç”Ÿå˜é‡åŠå…¶ isna æŒ‡ç¤ºï¼ˆåªä¿ç•™å­˜åœ¨çš„ï¼‰\n",
    "_CANDIDATE_EXOG = [\"traffic_level_mean_tomtom\", \"vessel_count_vessel\", \"vessel_avg_speed_vessel\"]\n",
    "EXOG_COLS = [c for c in _CANDIDATE_EXOG if c in df.columns]\n",
    "EXOG_ISNA_COLS = [f\"{c}_isna\" for c in _CANDIDATE_EXOG if f\"{c}_isna\" in df.columns]\n",
    "EXOG_ALL = EXOG_COLS + EXOG_ISNA_COLS  # ä¸€å¹¶é€å…¥æ¨¡å‹ï¼Œè®©æ ‘å­¦ä¹ â€œç¼ºå¤±â€ä¿¡æ¯\n",
    "\n",
    "# 6.4 è¦è®­ç»ƒçš„ä¼ æ„Ÿå™¨æ¸…å•ï¼ˆå¯å…ˆæŠ½æ ·ä¸€éƒ¨åˆ†è¿›è¡Œå¹²è·‘ï¼‰\n",
    "# TARGET_SENSORS = SENSOR_COLS[:5]   # åªè·‘å‰5ä¸ªåšå†’çƒŸæµ‹è¯•\n",
    "TARGET_SENSORS = SENSOR_COLS         # ç”Ÿäº§æ¨¡å¼ï¼šå…¨ä½“ä¼ æ„Ÿå™¨\n",
    "\n",
    "# ---------- 7) è¯„ä¼°ä¸æŒä¹…åŒ–æ–‡ä»¶ ----------\n",
    "METRICS_FILE = ARTIFACTS_DIR / \"metrics_summary.csv\"  # è®­ç»ƒè¯„ä¼°æ€»è¡¨\n",
    "OPTUNA_DB = ARTIFACTS_DIR / \"optuna_study.db\"         # Optuna RDBï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "# ---------- 8) å¿«é€Ÿæ—¥å¿— ----------\n",
    "logging.info(\n",
    "    f\"Rows: {len(df)} | Sensors: {len(SENSOR_COLS)} | \"\n",
    "    f\"Time features: {len(TIME_COLS)} | Exogenous: {len(EXOG_ALL)}\"\n",
    ")\n",
    "logging.info(f\"Sample sensors: {SENSOR_COLS[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ab711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_for_sensor(\n",
    "    df,\n",
    "    sensor_col: str,\n",
    "    max_lag: int,\n",
    "    exog_cols: list,\n",
    "    time_cols: list,\n",
    "    target_shift: int = 0,\n",
    "    add_rolling: bool = False,\n",
    "    rolling_windows: tuple = (3, 6, 12),\n",
    "    add_diff: bool = False,\n",
    "    exog_lags: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a time-series training dataset for one sensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The full dataframe. Index must be a DatetimeIndex (timestamp).\n",
    "    sensor_col : str\n",
    "        The name of the target sensor column (e.g. 'CMSA-GAKH-01__sensor').\n",
    "    max_lag : int\n",
    "        The maximum number of lag features to create (1..max_lag).\n",
    "    exog_cols : list[str]\n",
    "        List of exogenous variables and their _isna flags (will auto-filter only those present in df).\n",
    "    time_cols : list[str]\n",
    "        List of time-derived features (only those present in df will be used).\n",
    "    target_shift : int, default=0\n",
    "        How many steps ahead to predict (0 = current step, 1 = next timestamp, etc.).\n",
    "    add_rolling : bool, default=False\n",
    "        Whether to add rolling mean/std features for the target series.\n",
    "    rolling_windows : tuple[int], default=(3, 6, 12)\n",
    "        Window sizes for rolling features (each unit equals one sample interval).\n",
    "    add_diff : bool, default=False\n",
    "        Whether to include first-order differencing of the target.\n",
    "    exog_lags : int, default=0\n",
    "        Whether to include lagged versions of exogenous variables (0 = none).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        The feature matrix, fully aligned and cleaned.\n",
    "    y : pd.Series\n",
    "        The target vector, aligned with X.\n",
    "    feature_names : list[str]\n",
    "        List of feature names used in the model.\n",
    "    meta : dict\n",
    "        Metadata about the dataset (sensor, n_samples, max_lag, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 0) Basic validation ---\n",
    "    if sensor_col not in df.columns:\n",
    "        raise KeyError(f\"Target column {sensor_col} not found in dataframe.\")\n",
    "    if max_lag < 1:\n",
    "        raise ValueError(\"max_lag must be >= 1\")\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise TypeError(\"df.index must be a DatetimeIndex (set timestamp as index first).\")\n",
    "\n",
    "    # --- 1) Copy only relevant columns to avoid modifying original df ---\n",
    "    keep_cols = [sensor_col]\n",
    "    time_cols = [c for c in time_cols if c in df.columns]\n",
    "    exog_cols = [c for c in exog_cols if c in df.columns]\n",
    "    keep_cols += time_cols + exog_cols\n",
    "    work = df[keep_cols].copy()\n",
    "\n",
    "    # --- 2) Create lag features for the target variable ---\n",
    "    lag_cols = []\n",
    "    for k in range(1, max_lag + 1):\n",
    "        colk = f\"{sensor_col}_lag{k}\"\n",
    "        work[colk] = work[sensor_col].shift(k)\n",
    "        lag_cols.append(colk)\n",
    "\n",
    "    # --- 3) Add first-order difference (optional) ---\n",
    "    diff_cols = []\n",
    "    if add_diff:\n",
    "        cold = f\"{sensor_col}_diff1\"\n",
    "        work[cold] = work[sensor_col].diff(1)\n",
    "        diff_cols.append(cold)\n",
    "\n",
    "    # --- 4) Add rolling mean/std features (optional) ---\n",
    "    roll_cols = []\n",
    "    if add_rolling:\n",
    "        for w in rolling_windows:\n",
    "            cm = f\"{sensor_col}_rollmean{w}\"\n",
    "            cs = f\"{sensor_col}_rollstd{w}\"\n",
    "            work[cm] = work[sensor_col].rolling(window=w, min_periods=w).mean()\n",
    "            work[cs] = work[sensor_col].rolling(window=w, min_periods=w).std()\n",
    "            roll_cols += [cm, cs]\n",
    "\n",
    "    # --- 5) Add lagged versions of exogenous variables (optional) ---\n",
    "    exog_lag_cols = []\n",
    "    if exog_lags and exog_lags > 0:\n",
    "        for c in exog_cols:\n",
    "            for k in range(1, exog_lags + 1):\n",
    "                colk = f\"{c}_lag{k}\"\n",
    "                work[colk] = work[c].shift(k)\n",
    "                exog_lag_cols.append(colk)\n",
    "\n",
    "    # --- 6) Create target vector y ---\n",
    "    # Shift the target forward if predicting future values (target_shift > 0)\n",
    "    y = work[sensor_col].shift(-target_shift) if target_shift > 0 else work[sensor_col]\n",
    "\n",
    "    # --- 7) Define final feature order ---\n",
    "    feature_cols = []\n",
    "    feature_cols += time_cols          # time-based features\n",
    "    feature_cols += exog_cols          # exogenous variables + isna flags\n",
    "    feature_cols += exog_lag_cols      # lagged exogenous (if any)\n",
    "    feature_cols += lag_cols           # lagged target values\n",
    "    feature_cols += diff_cols          # difference features\n",
    "    feature_cols += roll_cols          # rolling features\n",
    "\n",
    "    X = work[feature_cols]\n",
    "\n",
    "    # --- 8) Align and clean missing values ---\n",
    "    # Lagging and rolling produce NaNs at the start; shifting target adds NaNs at the end.\n",
    "    # Drop any rows with missing data to keep X and y aligned.\n",
    "    data = pd.concat([X, y.rename(\"y\")], axis=1)\n",
    "    data = data.dropna(axis=0, how=\"any\")\n",
    "\n",
    "    # Align X and y\n",
    "    y = data[\"y\"]\n",
    "    X = data.drop(columns=[\"y\"])\n",
    "\n",
    "    # --- 9) Metadata for traceability ---\n",
    "    meta = {\n",
    "        \"sensor\": sensor_col,\n",
    "        \"max_lag\": max_lag,\n",
    "        \"target_shift\": target_shift,\n",
    "        \"n_samples\": len(X),\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"used_time_cols\": time_cols,\n",
    "        \"used_exog_cols\": exog_cols,\n",
    "        \"added_exog_lags\": exog_lags,\n",
    "        \"added_diff\": add_diff,\n",
    "        \"added_rolling\": add_rolling,\n",
    "        \"rolling_windows\": rolling_windows if add_rolling else (),\n",
    "    }\n",
    "\n",
    "    return X, y, feature_cols, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "156f49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_time_series_splits(\n",
    "    X,\n",
    "    y,\n",
    "    n_splits: int = 3,\n",
    "    test_size: float = 0.2,\n",
    "    gap: int = 0,\n",
    "    mode: str = \"expanding\",\n",
    "    min_train_ratio: float = 0.4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate multiple time-based splits (rolling or expanding window)\n",
    "    for time-series cross-validation with a gap before each validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix, must be time-ordered (DatetimeIndex or sorted index).\n",
    "    y : pd.Series\n",
    "        Target vector, aligned with X (same index).\n",
    "    n_splits : int, default=3\n",
    "        Number of folds (commonly 3â€“5).\n",
    "    test_size : float, default=0.2\n",
    "        Fraction of total samples to allocate for each validation set.\n",
    "    gap : int, default=0\n",
    "        Number of samples to skip between train and validation to prevent leakage.\n",
    "    mode : {\"expanding\", \"sliding\"}, default=\"expanding\"\n",
    "        - \"expanding\": training set starts at the beginning and grows each fold.\n",
    "        - \"sliding\" : fixed-length training window sliding forward.\n",
    "    min_train_ratio : float, default=0.4\n",
    "        Minimum proportion of samples for the first training window.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    (X_train, y_train, X_val, y_val, fold_info)\n",
    "        - X_train, y_train : training subset\n",
    "        - X_val, y_val     : validation subset\n",
    "        - fold_info : dict with metadata (fold index, sizes, date range)\n",
    "    \"\"\"\n",
    "    import math\n",
    "\n",
    "    # --- Basic validation ---\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(\"X and y must have the same length and aligned index.\")\n",
    "    n = len(X)\n",
    "    if n < 10:\n",
    "        raise ValueError(\"Too few samples for time-series splitting.\")\n",
    "    if not 0 < test_size < 1:\n",
    "        raise ValueError(\"test_size must be in (0, 1).\")\n",
    "\n",
    "    # --- Define sizes ---\n",
    "    min_train = max(1, int(n * min_train_ratio))\n",
    "    val_len = max(1, int(n * test_size))\n",
    "\n",
    "    for fold in range(1, n_splits + 1):\n",
    "        if mode == \"expanding\":\n",
    "            # Expanding window: train grows each fold\n",
    "            train_end = min_train + (fold - 1) * max(\n",
    "                1, (n - min_train - val_len) // max(1, n_splits)\n",
    "            )\n",
    "            val_start = train_end + gap\n",
    "            val_end = val_start + val_len\n",
    "\n",
    "        elif mode == \"sliding\":\n",
    "            # Sliding window: train window moves forward, fixed length\n",
    "            train_len = max(min_train, n - (n_splits - fold + 1) * (val_len + gap))\n",
    "            train_start = max(\n",
    "                0, n - (train_len + (n_splits - fold + 1) * (val_len + gap))\n",
    "            )\n",
    "            train_end = train_start + train_len\n",
    "            val_start = train_end + gap\n",
    "            val_end = val_start + val_len\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'expanding' or 'sliding'.\")\n",
    "\n",
    "        # Skip invalid ranges\n",
    "        if val_end > n or val_start >= n or train_end <= 0:\n",
    "            continue\n",
    "\n",
    "        # Slice the data (no shuffling)\n",
    "        X_tr = X.iloc[:train_end] if mode == \"expanding\" else X.iloc[train_start:train_end]\n",
    "        y_tr = y.iloc[:train_end] if mode == \"expanding\" else y.iloc[train_start:train_end]\n",
    "        X_val = X.iloc[val_start:val_end]\n",
    "        y_val = y.iloc[val_start:val_end]\n",
    "\n",
    "        # Skip empty splits\n",
    "        if len(X_val) == 0 or len(X_tr) == 0:\n",
    "            continue\n",
    "\n",
    "        # Metadata for logging\n",
    "        fold_info = {\n",
    "            \"fold\": fold,\n",
    "            \"mode\": mode,\n",
    "            \"gap\": gap,\n",
    "            \"n_train\": len(X_tr),\n",
    "            \"n_val\": len(X_val),\n",
    "            \"train_end_idx\": train_end,\n",
    "            \"val_start_idx\": val_start,\n",
    "            \"val_end_idx\": val_end,\n",
    "            \"train_index_start\": X_tr.index[0],\n",
    "            \"train_index_end\": X_tr.index[-1],\n",
    "            \"val_index_start\": X_val.index[0],\n",
    "            \"val_index_end\": X_val.index[-1],\n",
    "        }\n",
    "\n",
    "        yield X_tr, y_tr, X_val, y_val, fold_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be2f4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_sensor(\n",
    "    sensor_col: str,\n",
    "    df,\n",
    "    cfg,\n",
    "    exog_cols,\n",
    "    time_cols,\n",
    "    xgb_base_params,\n",
    "    study_db=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate one sensor model with multi-fold time-series CV and Optuna tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sensor_col : str\n",
    "        The sensor column name (e.g., 'CMSA-GAKH-01__sensor').\n",
    "    df : pd.DataFrame\n",
    "        Full dataset (must include all exogenous/time features and the target sensor column).\n",
    "    cfg : dict\n",
    "        Global configuration dictionary (contains optuna_n_trials, test_ratio, default_max_lag, etc.).\n",
    "    exog_cols : list[str]\n",
    "        List of exogenous variable names and _isna flags.\n",
    "    time_cols : list[str]\n",
    "        List of time-based feature names.\n",
    "    xgb_base_params : dict\n",
    "        Base XGBoost parameters (Optuna will search around them).\n",
    "    study_db : Path or None, optional\n",
    "        Path to an Optuna SQLite study DB (if None, a temporary in-memory study will be used).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Aggregated metrics across folds (mean R2, RMSE, MAE, best_params, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Build dataset\n",
    "    # --------------------------------------------------\n",
    "    X, y, feat_names, meta = build_dataset_for_sensor(\n",
    "        df=df,\n",
    "        sensor_col=sensor_col,\n",
    "        max_lag=cfg[\"default_max_lag\"],\n",
    "        exog_cols=exog_cols,\n",
    "        time_cols=time_cols,\n",
    "        target_shift=cfg[\"target_shift\"],\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Define Optuna objective\n",
    "    # --------------------------------------------------\n",
    "    def objective(trial):\n",
    "        # Search space around base parameters\n",
    "        params = xgb_base_params.copy()\n",
    "        params.update(\n",
    "            {\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "                \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                \"max_lag\": trial.suggest_int(\"max_lag\", 3, 12),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Rebuild dataset if lag length changes\n",
    "        X_trial, y_trial, _, _ = build_dataset_for_sensor(\n",
    "            df=df,\n",
    "            sensor_col=sensor_col,\n",
    "            max_lag=params[\"max_lag\"],\n",
    "            exog_cols=exog_cols,\n",
    "            time_cols=time_cols,\n",
    "            target_shift=cfg[\"target_shift\"],\n",
    "        )\n",
    "\n",
    "        # Multi-fold CV evaluation\n",
    "        scores_r2, scores_rmse, scores_mae = [], [], []\n",
    "        gap = params[\"max_lag\"] if cfg.get(\"gap_equals_max_lag\", True) else 0\n",
    "\n",
    "        for X_tr, y_tr, X_val, y_val, fi in rolling_time_series_splits(\n",
    "            X_trial,\n",
    "            y_trial,\n",
    "            n_splits=3,\n",
    "            test_size=cfg[\"test_ratio\"],\n",
    "            gap=gap,\n",
    "            mode=\"expanding\",\n",
    "            min_train_ratio=0.4,\n",
    "        ):\n",
    "            params_with_metric = params.copy()\n",
    "            params_with_metric[\"eval_metric\"] = \"rmse\"\n",
    "            \n",
    "            allowed_keys = {\n",
    "                \"n_estimators\", \"max_depth\", \"learning_rate\",\n",
    "                \"subsample\", \"colsample_bytree\",\n",
    "                \"reg_lambda\", \"reg_alpha\", \"min_child_weight\",\n",
    "                \"gamma\", \"max_bin\", \"tree_method\",\n",
    "                \"n_jobs\", \"random_state\", \"eval_metric\"\n",
    "            }\n",
    "            model_params = {k: v for k, v in params_with_metric.items() if k in allowed_keys}\n",
    "\n",
    "            model = xgb.XGBRegressor(**model_params)\n",
    "\n",
    "            model.fit(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=False,\n",
    "            )  \n",
    "\n",
    "            preds = model.predict(X_val)\n",
    "            scores_r2.append(r2_score(y_val, preds))\n",
    "            scores_rmse.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "            scores_mae.append(mean_absolute_error(y_val, preds))\n",
    "\n",
    "        # Average fold results (optimize by RMSE)\n",
    "        mean_rmse = np.mean(scores_rmse)\n",
    "        trial.set_user_attr(\"r2\", np.mean(scores_r2))\n",
    "        trial.set_user_attr(\"mae\", np.mean(scores_mae))\n",
    "        return mean_rmse\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3) Run Optuna optimization\n",
    "    # --------------------------------------------------\n",
    "    study_name = f\"xgb_{sensor_col}\"\n",
    "    storage = f\"sqlite:///{study_db}\" if study_db is not None else None\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=cfg[\"seed\"]),\n",
    "        pruner=optuna.pruners.MedianPruner(),\n",
    "        storage=storage,\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    study.optimize(objective, n_trials=cfg[\"optuna_n_trials\"], timeout=cfg[\"trial_timeout_sec\"])\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_rmse = study.best_value\n",
    "    best_r2 = study.best_trial.user_attrs.get(\"r2\", None)\n",
    "    best_mae = study.best_trial.user_attrs.get(\"mae\", None)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4) Retrain final model on all data with best params\n",
    "    # --------------------------------------------------\n",
    "    final_params = xgb_base_params.copy()\n",
    "    final_params.update(best_params)\n",
    "    final_params[\"eval_metric\"] = \"rmse\"\n",
    "\n",
    "    allowed_keys = {\n",
    "        \"n_estimators\", \"max_depth\", \"learning_rate\",\n",
    "        \"subsample\", \"colsample_bytree\",\n",
    "        \"reg_lambda\", \"reg_alpha\", \"min_child_weight\",\n",
    "        \"gamma\", \"max_bin\", \"tree_method\",\n",
    "        \"n_jobs\", \"random_state\", \"eval_metric\"\n",
    "    }\n",
    "\n",
    "    final_model_params = {k: v for k, v in final_params.items() if k in allowed_keys}\n",
    "    final_model = xgb.XGBRegressor(**final_model_params)\n",
    "    final_model.fit(X, y)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5) Save model and record metrics\n",
    "    # --------------------------------------------------\n",
    "    model_path = MODELS_DIR / f\"{sensor_col}.json\"\n",
    "    final_model.save_model(model_path)\n",
    "\n",
    "    metrics = {\n",
    "        \"sensor\": sensor_col,\n",
    "        \"r2_mean\": best_r2,\n",
    "        \"rmse_mean\": best_rmse,\n",
    "        \"mae_mean\": best_mae,\n",
    "        \"n_samples\": len(X),\n",
    "        \"n_features\": X.shape[1],\n",
    "        \"best_params\": best_params,\n",
    "        \"model_path\": str(model_path),\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ab94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 16:41:27,689 | INFO | ğŸš€ Starting batch training for 37 sensors...\n",
      "2025-11-05 16:41:27,690 | INFO | \n",
      "==============================\n",
      "2025-11-05 16:41:27,691 | INFO | [1/37] Training sensor: CMSA-GAKH-01__sensor\n",
      "2025-11-05 16:42:38,283 | ERROR | âŒ CMSA-GAKH-01__sensor failed: 'train_time_sec'\n",
      "2025-11-05 16:42:38,285 | INFO | \n",
      "==============================\n",
      "2025-11-05 16:42:38,285 | INFO | [2/37] Training sensor: CMSA-GAWW-11__sensor\n",
      "2025-11-05 16:43:51,955 | ERROR | âŒ CMSA-GAWW-11__sensor failed: 'train_time_sec'\n",
      "2025-11-05 16:43:51,956 | INFO | \n",
      "==============================\n",
      "2025-11-05 16:43:51,956 | INFO | [3/37] Training sensor: CMSA-GAWW-12__sensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 failed with parameters: {'max_depth': 7, 'learning_rate': 0.11114989443094977, 'subsample': 0.608233797718321, 'colsample_bytree': 0.9879639408647978, 'reg_lambda': 2.1368329072358767, 'reg_alpha': 0.0070689749506246055, 'min_child_weight': 2, 'max_lag': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\elvinli\\AppData\\Local\\Temp\\ipykernel_37504\\3550949636.py\", line 108, in objective\n",
      "    model.fit(\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1247, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\training.py\", line 192, in train\n",
      "    return bst.reset()\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py\", line 2076, in reset\n",
      "    _check_call(_LIB.XGBoosterReset(self.handle))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m start_time = time.time()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# è°ƒç”¨å•ä¼ æ„Ÿå™¨è®­ç»ƒå°è£…å‡½æ•°\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     metrics = \u001b[43mtrain_one_sensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43msensor_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43msensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexog_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEXOG_ALL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtime_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTIME_COLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxgb_base_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mXGB_BASE_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy_db\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOPTUNA_DB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     elapsed = time.time() - start_all\n\u001b[32m     39\u001b[39m     avg = elapsed / i\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 139\u001b[39m, in \u001b[36mtrain_one_sensor\u001b[39m\u001b[34m(sensor_col, df, cfg, exog_cols, time_cols, xgb_base_params, study_db)\u001b[39m\n\u001b[32m    130\u001b[39m storage = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy_db\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m study_db \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    131\u001b[39m study = optuna.create_study(\n\u001b[32m    132\u001b[39m     study_name=study_name,\n\u001b[32m    133\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m     load_if_exists=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    138\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptuna_n_trials\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrial_timeout_sec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m best_params = study.best_trial.params\n\u001b[32m    142\u001b[39m best_rmse = study.best_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mtrain_one_sensor.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    104\u001b[39m model_params = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params_with_metric.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m allowed_keys}\n\u001b[32m    106\u001b[39m model = xgb.XGBRegressor(**model_params)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m    115\u001b[39m preds = model.predict(X_val)\n\u001b[32m    116\u001b[39m scores_r2.append(r2_score(y_val, preds))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\training.py:192\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evals_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     evals_result.update(cb_container.history)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvinli\\.conda\\envs\\TIL6022-25\\Lib\\site-packages\\xgboost\\core.py:2076\u001b[39m, in \u001b[36mBooster.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mBooster\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2071\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reset the booster object to release data caches used for training.\u001b[39;00m\n\u001b[32m   2072\u001b[39m \n\u001b[32m   2073\u001b[39m \u001b[33;03m    .. versionadded:: 3.0.0\u001b[39;00m\n\u001b[32m   2074\u001b[39m \n\u001b[32m   2075\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2076\u001b[39m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterReset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2077\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# æ‰¹é‡è®­ç»ƒå¾ªç¯ï¼ˆBatch Training Loopï¼‰\n",
    "# ============================================================\n",
    "\n",
    "# è¯´æ˜ï¼š\n",
    "# - ä¾æ¬¡éå† TARGET_SENSORS ä¸­çš„æ¯ä¸ªä¼ æ„Ÿå™¨\n",
    "# - è°ƒç”¨ train_one_sensor() è¿›è¡Œå¤šæŠ˜è°ƒå‚ä¸è®­ç»ƒ\n",
    "# - å°†æ¯ä¸ªä¼ æ„Ÿå™¨çš„ç»“æœè¿½åŠ ä¿å­˜åˆ° metrics_summary.csv\n",
    "# - è‹¥ä¸ªåˆ«ä¼ æ„Ÿå™¨è®­ç»ƒå¤±è´¥ï¼Œä¸å½±å“æ•´ä½“æµç¨‹\n",
    "import time\n",
    "\n",
    "# åˆå§‹åŒ–ç»“æœè®°å½•è¡¨\n",
    "metrics_records = []\n",
    "\n",
    "# æ‰“å°æ€»ä½“ä»»åŠ¡ä¿¡æ¯\n",
    "logging.info(f\"ğŸš€ Starting batch training for {len(TARGET_SENSORS)} sensors...\")\n",
    "\n",
    "start_all = time.time()\n",
    "\n",
    "for i, sensor in enumerate(TARGET_SENSORS, start=1):\n",
    "    logging.info(f\"\\n==============================\")\n",
    "    logging.info(f\"[{i}/{len(TARGET_SENSORS)}] Training sensor: {sensor}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # è°ƒç”¨å•ä¼ æ„Ÿå™¨è®­ç»ƒå°è£…å‡½æ•°\n",
    "        metrics = train_one_sensor(\n",
    "            sensor_col=sensor,\n",
    "            df=df,\n",
    "            cfg=CFG,\n",
    "            exog_cols=EXOG_ALL,\n",
    "            time_cols=TIME_COLS,\n",
    "            xgb_base_params=XGB_BASE_PARAMS,\n",
    "            study_db=OPTUNA_DB,\n",
    "        )\n",
    "    \n",
    "        # å¢åŠ æ—¶é—´ä¿¡æ¯ä¸çŠ¶æ€\n",
    "        metrics[\"status\"] = \"success\"\n",
    "        metrics[\"train_time_sec\"] = round(time.time() - start_time, 2)\n",
    "        metrics_records.append(metrics)\n",
    "\n",
    "        logging.info(\n",
    "            f\"âœ… {sensor} finished | \"\n",
    "            f\"RÂ²={metrics.get('r2_mean'):.4f}, RMSE={metrics.get('rmse_mean'):.4f}, \"\n",
    "            f\"MAE={metrics.get('mae_mean'):.4f}, Time={metrics['train_time_sec']}s\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        # è®­ç»ƒå¤±è´¥çš„ä¼ æ„Ÿå™¨ä¼šè¢«è®°å½•ä¸‹æ¥\n",
    "        logging.error(f\"âŒ {sensor} failed: {e}\")\n",
    "        metrics_records.append({\n",
    "            \"sensor\": sensor,\n",
    "            \"status\": \"failed\",\n",
    "            \"error\": str(e),\n",
    "            \"r2_mean\": None,\n",
    "            \"rmse_mean\": None,\n",
    "            \"mae_mean\": None,\n",
    "            \"train_time_sec\": round(time.time() - start_time, 2)\n",
    "        })\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# æ±‡æ€»ä¿å­˜ç»“æœ\n",
    "# ------------------------------------------------------------\n",
    "df_metrics = pd.DataFrame(metrics_records)\n",
    "df_metrics.to_csv(METRICS_FILE, index=False)\n",
    "\n",
    "total_time = round(time.time() - start_all, 2)\n",
    "logging.info(f\"\\nğŸ All sensors completed in {total_time}s\")\n",
    "logging.info(f\"Results saved to: {METRICS_FILE}\")\n",
    "\n",
    "# å¯é€‰ï¼šæ‰“å°è®­ç»ƒæˆåŠŸ/å¤±è´¥ç»Ÿè®¡\n",
    "n_success = (df_metrics[\"status\"] == \"success\").sum()\n",
    "n_failed = (df_metrics[\"status\"] == \"failed\").sum()\n",
    "logging.info(f\"Training summary â†’ Success: {n_success}, Failed: {n_failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a08d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No successful models found â€” please check training results.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m dfm_success = dfm[dfm[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m].copy()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dfm_success.empty:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo successful models found â€” please check training results.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- 2) åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ ---\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Loaded metrics summary:\u001b[39m\u001b[33m\"\u001b[39m, metrics_path)\n",
      "\u001b[31mValueError\u001b[39m: No successful models found â€” please check training results."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# æ¨¡å‹æ€§èƒ½å¯è§†åŒ–è„šæœ¬ (Visualization of model performance)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1) è¯»å–ç»“æœæ–‡ä»¶ ---\n",
    "metrics_path = METRICS_FILE\n",
    "if not metrics_path.exists():\n",
    "    raise FileNotFoundError(f\"Metrics file not found: {metrics_path}\")\n",
    "\n",
    "dfm = pd.read_csv(metrics_path)\n",
    "dfm_success = dfm[dfm[\"status\"] == \"success\"].copy()\n",
    "\n",
    "if dfm_success.empty:\n",
    "    raise ValueError(\"No successful models found â€” please check training results.\")\n",
    "\n",
    "# --- 2) åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ ---\n",
    "print(\"âœ… Loaded metrics summary:\", metrics_path)\n",
    "print(f\"Successful models: {len(dfm_success)} / {len(dfm)}\")\n",
    "print(f\"Average RÂ²: {dfm_success['r2_mean'].mean():.3f}\")\n",
    "print(f\"Average RMSE: {dfm_success['rmse_mean'].mean():.3f}\")\n",
    "print(f\"Average MAE: {dfm_success['mae_mean'].mean():.3f}\")\n",
    "\n",
    "# --- 3) ç»Ÿä¸€æ ·å¼è®¾ç½® ---\n",
    "sns.set(style=\"whitegrid\", font=\"DejaVu Sans\", rc={\"axes.titlesize\": 14, \"axes.labelsize\": 12})\n",
    "\n",
    "# --- 4) RÂ² æ’åæ¡å½¢å›¾ ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_plot = dfm_success.sort_values(\"r2_mean\", ascending=False)\n",
    "sns.barplot(x=\"r2_mean\", y=\"sensor\", data=df_plot, palette=\"viridis\")\n",
    "plt.title(\"Model Performance by Sensor (RÂ² Score)\")\n",
    "plt.xlabel(\"RÂ² (higher is better)\")\n",
    "plt.ylabel(\"Sensor ID\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"sensor_r2_ranking.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 5) RMSE æ’åæ¡å½¢å›¾ ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_plot = dfm_success.sort_values(\"rmse_mean\", ascending=True)\n",
    "sns.barplot(x=\"rmse_mean\", y=\"sensor\", data=df_plot, palette=\"rocket_r\")\n",
    "plt.title(\"Model RMSE by Sensor (lower is better)\")\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.ylabel(\"Sensor ID\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"sensor_rmse_ranking.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 6) MAE æ’åæ¡å½¢å›¾ ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_plot = dfm_success.sort_values(\"mae_mean\", ascending=True)\n",
    "sns.barplot(x=\"mae_mean\", y=\"sensor\", data=df_plot, palette=\"crest\")\n",
    "plt.title(\"Model MAE by Sensor (lower is better)\")\n",
    "plt.xlabel(\"MAE\")\n",
    "plt.ylabel(\"Sensor ID\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"sensor_mae_ranking.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 7) æŒ‡æ ‡åˆ†å¸ƒç®±çº¿å›¾ ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=dfm_success[[\"r2_mean\", \"rmse_mean\", \"mae_mean\"]], palette=\"Set2\")\n",
    "plt.title(\"Distribution of Model Performance Metrics\")\n",
    "plt.ylabel(\"Metric value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"metrics_distribution_boxplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 8) è¾“å‡º Top-N æœ€ä½³ä¸æœ€å·®æ¨¡å‹è¡¨æ ¼ ---\n",
    "TOP_N = 5\n",
    "print(\"\\nğŸ† Top models by RÂ²:\")\n",
    "display(dfm_success.nlargest(TOP_N, \"r2_mean\")[[\"sensor\", \"r2_mean\", \"rmse_mean\", \"mae_mean\"]])\n",
    "\n",
    "print(\"\\nâš ï¸ Worst models by RÂ²:\")\n",
    "display(dfm_success.nsmallest(TOP_N, \"r2_mean\")[[\"sensor\", \"r2_mean\", \"rmse_mean\", \"mae_mean\"]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
